{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, TensorDataset\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set de datos\n",
    "Antes de armar la red neuronal vamos a importar los datos que se van a requerir usando pandas. Vamos a hacer un preprocesamiento para que el formato sea compatible con la red neuronal que armaremos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "datos = pd.read_csv(\"Churn_Modelling.csv\")\n",
    "datos.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imprimiento los primeros 5 elementos podemos ver que nuestra variable destino es la ultima columna **Exited** donde 1 es que se dio de baja y 0 que se quedó.\n",
    "\n",
    "Es importante que en este paso vayamos pensando que tendremos que hacer con algunas columnas, ya sea eliminarlas, normalizarlas, llevar alguna transformación etc. Por ejemplo:\n",
    "* RowNumber, CustomerID, Surname no nos dicen nada de utilidad. Predecir si alguien va a darse de baja por su ID de cliente, apellido o numero de renglon en la base de datos no nos va a dar buenos resultados. \n",
    "* Las columnas que son categoricas las vamos a transformar con **one hot encoding** (Genero y ubicacion geográfica)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Separamos la ultima columna para que sea variable destino\n",
    "datos_y = datos[datos.columns[-1]]\n",
    "datos_y.head()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Eliminamos las columnas que no funcionarán\n",
    "datos_x = datos.drop(columns=[\"RowNumber\", \"CustomerId\", \"Surname\"])\n",
    "datos_x.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Convertimos en one hot encoding las columnas de genero y zona geográfica\n",
    "datos_x = pd.get_dummies(datos_x)\n",
    "datos_x = datos_x.drop(columns=[\"Exited\"])\n",
    "datos_x.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ya se cuenta con una variable que tiene todas las entradas al modelo **datos_x** y otra con la salida **datos_y**. Para datos_x se cuenta con una columna por cada categoria de las variables Genero y Zona geofráfica.\n",
    "\n",
    "# Dividir datos entre entrenamiento y test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "datos_x.shape[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# X_train = datos_x[:int(datos_x.shape[0]*.8)] \n",
    "# X_test = datos_x[:int(datos_x.shape[0]*.2)]\n",
    "# y_train = datos_y[:int(datos_y.shape[0]*0.8)]\n",
    "# y_test = datos_y[:int(datos_y.shape[0]*0.2)]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(datos_x, datos_y, test_size = 0.2, random_state = 42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"X Train: {}, X Test: {}, y_train: {}, y_test: {}\".format(X_train.shape, X_test.shape, y_train.shape, y_test.shape))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "entradas = X_train.shape[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Escalado datos\n",
    "Ahora vamos a escalar los valores para que esten dentro de un rango mas corto."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "escalador = StandardScaler()\n",
    "X_train = escalador.fit_transform(X_train)\n",
    "X_test = escalador.fit_transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tensores\n",
    "Para poder procesar los datos en la red neuronal necesitamos que todos los datos estén en tensores, asi que haremos las conversiones necesarias"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "t_X_train = torch.from_numpy(X_train).float().to(\"cpu\")\n",
    "t_X_test = torch.from_numpy(X_test).float().to(\"cpu\")\n",
    "t_y_train = torch.from_numpy(y_train.values).float().to(\"cpu\")\n",
    "t_y_test = torch.from_numpy(y_test.values).float().to(\"cpu\")\n",
    "t_y_train = t_y_train[:,None]\n",
    "t_y_test = t_y_test[:, None]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test = TensorDataset(t_X_test, t_y_test)\n",
    "print(test[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "t_y_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Estructura de la red neuronal\n",
    "Ahora vamos a armar una estructura básica de una red neuronal la cual va a recibir los datos de **X** para eventualmente poder predecir **y**\n",
    "\n",
    "Para hacer esto tenemos que crear una Clase la cual hereda de nn.Module de torch."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, entradas):\n",
    "        super(Network, self).__init__()\n",
    "        self.linear1 = nn.Linear(entradas, 15)\n",
    "        self.linear2 = nn.Linear(15, 8)\n",
    "        self.linear3 = nn.Linear(8,160)\n",
    "        self.linear4 = nn.Linear(160, 200)\n",
    "        self.linear5 = nn.Linear(200, 1)\n",
    "        # self.linear3 = nn.Linear(8, 1)\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        prediction = torch.sigmoid(input=self.linear1(xb))\n",
    "        prediction = torch.sigmoid(input=self.linear2(prediction))\n",
    "        prediction = torch.sigmoid(input=self.linear3(prediction))\n",
    "        prediction = torch.sigmoid(input=self.linear4(prediction))\n",
    "        prediction = torch.sigmoid(input=self.linear5(prediction))\n",
    "        # prediction = torch.sigmoid(input=self.linear3(prediction))\n",
    "        return prediction"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "t_y_test[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#float(len(t_y_test))\n",
    "#y_pred # Tensor con valores de 0 a 1\n",
    "#y_pred_class  # Tensor de 0 y 1\n",
    "\n",
    "t_sum = t_y_test.sum()  #461\n",
    "t_len = len(t_y_test) #2000\n",
    "calculo = 416/2000 #0.2305\n",
    "eq_es = t_y_test.sum()/float(len(t_y_test))    #0.2080\n",
    "#print(\"t_y_sum: {}, t_y_len: {}, calculo: {}, eq: {}\".format(t_sum, t_len, calculo, eq_es))\n",
    "#print(y_pred_class.eq(eq_es))\n",
    "correct = (y_pred_class == t_y_test).sum()\n",
    "print(correct)\n",
    "t_y_test\n",
    "indice = 568\n",
    "print(y_pred_class[indice], t_y_test[indice])\n",
    "print(\"La prediccion es: {}     El valor real: {}\".format(model(t_X_test[indice]), t_y_test[indice]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "lr = 0.01\n",
    "model = Network(entradas=entradas)\n",
    "print(f\"Model Architecture:\\n{model}\")\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "\n",
    "nb_epochs = 1200\n",
    "print_offset = 100\n",
    "\n",
    "df_tracker = pd.DataFrame()\n",
    "print(\"\\nTraining the model...\")\n",
    "for epoch in range(1, nb_epochs+1):\n",
    "    y_pred = model(t_X_train)\n",
    "    #print(y_pred.shape)\n",
    "    loss = criterion(input=y_pred, target=t_y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % print_offset == 0:\n",
    "        print(f\"\\nEpoch {epoch} \\t Loss: {round(loss.item(), 4)}\")\n",
    "    \n",
    "    # Print test-accuracy after certain number of epochs\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(t_X_test)\n",
    "        y_pred_class = y_pred.round()\n",
    "        #print(y_pred_class.shape)\n",
    "        # accuracy = y_pred_class.eq(t_y_test).sum() / float(len(t_y_test))\n",
    "        correct = (y_pred_class == t_y_test).sum()\n",
    "        accuracy = 100 * correct / float(len(t_y_test))\n",
    "        if epoch % print_offset == 0:\n",
    "            print(\"Pred_Clas: {}, Y_test\".format(t_y_test.sum(), len(t_y_test)))\n",
    "            print(f\"Accuracy (on test-set): {round(accuracy.item(), 4)}\")\n",
    "    \n",
    "    df_temp = pd.DataFrame(data={\n",
    "        'Epoch': epoch,\n",
    "        'Loss': round(loss.item(), 4),\n",
    "        'Accuracy': round(accuracy.item(), 4)\n",
    "    }, index=[0])\n",
    "    df_tracker = pd.concat(objs=[df_tracker, df_temp], ignore_index=True, sort=False)\n",
    "\n",
    "print(f\"\\nFinal Accuracy (on test-set): {round(accuracy.item(), 4)}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_tracker['Epoch'], df_tracker['Loss'], color='purple', linewidth=2, label='Loss')\n",
    "plt.title(\"Loss over time\", fontsize=25)\n",
    "plt.xlabel(\"Epoch\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.grid()\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_tracker['Epoch'], df_tracker['Accuracy'], color='blue', linewidth=2, label='Accuracy')\n",
    "plt.title(\"Accuracy over time\", fontsize=25)\n",
    "plt.xlabel(\"Epoch\", fontsize=16)\n",
    "plt.ylabel(\"Accuracy\", fontsize=16)\n",
    "plt.grid()\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "97b669972ed46336d3ec5c9253905be09aadac6dbd287963ff3ed96ab29d7383"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}